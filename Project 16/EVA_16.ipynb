{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EVA_16.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "Uw5QRZFnozQd"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wp7vQuVKugu",
        "colab_type": "text"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVIx_KIigxPV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.datasets import cifar10\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "import numpy as np\n",
        "% matplotlib inline \n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D,AveragePooling2D, GlobalAveragePooling2D, GlobalMaxPooling2D\n",
        "from tensorflow.keras.layers import Input, Activation, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Add, Concatenate\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint, Callback\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.python.keras.utils import data_utils\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNHw6luQg3gc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "K.set_session(tf.Session(config=config))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BR-MZfPjKyh4",
        "colab_type": "text"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsO_yGxcg5D8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyperparameters\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 50\n",
        "l = 8\n",
        "num_filter = 16\n",
        "compression = 0.9\n",
        "dropout_rate = 0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkgaSC2mK8F1",
        "colab_type": "text"
      },
      "source": [
        "### Clone API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfJxl0IjK-jg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8a6d9bbb-1814-42a4-b30a-5a27024aaba2"
      },
      "source": [
        "!git clone https://github.com/prateekgulati/Jnana.git\n",
        "from Jnana.Data import convert_to_tfrecord_data,create_dataset"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'Jnana' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0Q4g-qXLh6Y",
        "colab_type": "text"
      },
      "source": [
        "### Read and Write TFRecord"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odMyYwTKiOTp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load CIFAR10 Data\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "img_height, img_width, channel = x_train.shape[1],x_train.shape[2],x_train.shape[3]\n",
        "STEPS_PER_EPOCH = len(x_train) // batch_size\n",
        "VALIDATION_STEPS = len(x_test) // batch_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mB7o3zu1g6eT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_mean = np.mean(x_train, axis=(0,1,2))\n",
        "# train_std = np.std(x_train, axis=(0,1,2))\n",
        "\n",
        "# test_mean = np.mean(x_train, axis=(0,1,2))\n",
        "# test_std = np.std(x_train, axis=(0,1,2))\n",
        "\n",
        "# normalize = lambda x: ((x - train_mean) / train_std).astype('float32')\n",
        "# normalize_test = lambda x: ((x - test_mean) / test_std).astype('float32')\n",
        "\n",
        "# x_train = normalize(x_train)\n",
        "# x_test = normalize(x_test)\n",
        "\n",
        "# # convert to one hot encoing \n",
        "# y_train = utils.to_categorical(y_train, num_classes)\n",
        "# y_test = utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFzIAFwchI72",
        "colab_type": "code",
        "outputId": "000d35f8-0fed-4bbb-daa8-b4227716f140",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "convert_to_tfrecord_data(x_train,y_train, './train.tfrecord')\n",
        "convert_to_tfrecord_data(x_test,y_test, './test.tfrecord')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generating ./train.tfrecord\n",
            "Generating ./test.tfrecord\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXuOB8hbG6bI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_trainTF, y_trainTF = create_dataset('./train.tfrecord')\n",
        "x_testTF, y_testTF = create_dataset('./test.tfrecord')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1l7KVPBLZB50",
        "colab_type": "text"
      },
      "source": [
        "### Functions for Post Training Analysis "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmfsk76-fadV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_model_history(model_history):\n",
        "    fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
        "    # summarize history for accuracy\n",
        "    axs[0].plot(range(1,len(model_history.history['acc'])+1),model_history.history['acc'])\n",
        "    axs[0].plot(range(1,len(model_history.history['val_acc'])+1),model_history.history['val_acc'])\n",
        "    axs[0].set_title('Model Accuracy')\n",
        "    axs[0].set_ylabel('Accuracy')\n",
        "    axs[0].set_xlabel('Epoch')\n",
        "    axs[0].set_xticks(np.arange(1,len(model_history.history['acc'])+1),len(model_history.history['acc'])/10)\n",
        "    axs[0].legend(['train', 'val'], loc='best')\n",
        "    # summarize history for loss\n",
        "    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n",
        "    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n",
        "    axs[1].set_title('Model Loss')\n",
        "    axs[1].set_ylabel('Loss')\n",
        "    axs[1].set_xlabel('Epoch')\n",
        "    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)\n",
        "    axs[1].legend(['train', 'val'], loc='best')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJMT4rjgfdZz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(test_x, test_y, model):\n",
        "    result = model.predict(test_x)\n",
        "    predicted_class = np.argmax(result, axis=1)\n",
        "    true_class = np.argmax(test_y, axis=1)\n",
        "    num_correct = np.sum(predicted_class == true_class) \n",
        "    accuracy = float(num_correct)/result.shape[0]\n",
        "    return (accuracy * 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBwfxGXCL4QA",
        "colab_type": "text"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ee-sge5Kg7vr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dense Block\n",
        "def add_denseblock(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression\n",
        "    temp = input\n",
        "    for _ in range(l):\n",
        "        BatchNorm = BatchNormalization()(temp)\n",
        "        relu = Activation('relu')(BatchNorm)\n",
        "        Conv2D_3_3 = Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)\n",
        "        if dropout_rate>0:\n",
        "          Conv2D_3_3 = Dropout(dropout_rate)(Conv2D_3_3)\n",
        "        concat = Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
        "        \n",
        "        temp = concat\n",
        "        \n",
        "    return temp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOP6IPsGhBwb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_transition(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression\n",
        "    BatchNorm = BatchNormalization()(input)\n",
        "    relu = Activation('relu')(BatchNorm)\n",
        "    Conv2D_BottleNeck = Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
        "    if dropout_rate>0:\n",
        "      Conv2D_BottleNeck = Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
        "    avg = AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
        "    \n",
        "    return avg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RaKFpubhDIC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def output_layer(input):\n",
        "    global compression\n",
        "    BatchNorm = BatchNormalization()(input)\n",
        "    relu = Activation('relu')(BatchNorm)\n",
        "    AvgPooling = AveragePooling2D(pool_size=(2,2))(relu)\n",
        "    flat = Flatten()(AvgPooling)\n",
        "    output = Dense(num_classes, activation='softmax')(flat)\n",
        "    \n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anPCpQWhhGb7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input = Input(shape=(img_height, img_width, channel,))\n",
        "First_Conv2D = Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n",
        "\n",
        "First_Block = add_denseblock(First_Conv2D, num_filter, dropout_rate)\n",
        "First_Transition = add_transition(First_Block, num_filter, dropout_rate)\n",
        "\n",
        "Second_Block = add_denseblock(First_Transition, num_filter, dropout_rate)\n",
        "Second_Transition = add_transition(Second_Block, num_filter, dropout_rate)\n",
        "\n",
        "Third_Block = add_denseblock(Second_Transition, num_filter, dropout_rate)\n",
        "Third_Transition = add_transition(Third_Block, num_filter, dropout_rate)\n",
        "\n",
        "Last_Block = add_denseblock(Third_Transition,  num_filter, dropout_rate)\n",
        "output = output_layer(Last_Block)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uw5QRZFnozQd",
        "colab_type": "text"
      },
      "source": [
        "### Model Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kFh7pdxhNtT",
        "colab_type": "code",
        "outputId": "a98bea73-6a4a-431f-f5b9-2a4894a33eda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = Model(inputs=[input], outputs=[output])\n",
        "model.summary()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 32, 32, 16)   432         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 32, 32, 16)   64          conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 32, 32, 16)   0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 32, 32, 14)   2016        activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_35 (Dropout)            (None, 32, 32, 14)   0           conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_32 (Concatenate)    (None, 32, 32, 30)   0           conv2d_36[0][0]                  \n",
            "                                                                 dropout_35[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 32, 32, 30)   120         concatenate_32[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 32, 32, 30)   0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 32, 32, 14)   3780        activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_36 (Dropout)            (None, 32, 32, 14)   0           conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_33 (Concatenate)    (None, 32, 32, 44)   0           concatenate_32[0][0]             \n",
            "                                                                 dropout_36[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 32, 32, 44)   176         concatenate_33[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 32, 32, 44)   0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 32, 32, 14)   5544        activation_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_37 (Dropout)            (None, 32, 32, 14)   0           conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_34 (Concatenate)    (None, 32, 32, 58)   0           concatenate_33[0][0]             \n",
            "                                                                 dropout_37[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 32, 32, 58)   232         concatenate_34[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 32, 32, 58)   0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 32, 32, 14)   7308        activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_38 (Dropout)            (None, 32, 32, 14)   0           conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_35 (Concatenate)    (None, 32, 32, 72)   0           concatenate_34[0][0]             \n",
            "                                                                 dropout_38[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 32, 32, 72)   288         concatenate_35[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 32, 32, 72)   0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 32, 32, 14)   9072        activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_39 (Dropout)            (None, 32, 32, 14)   0           conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_36 (Concatenate)    (None, 32, 32, 86)   0           concatenate_35[0][0]             \n",
            "                                                                 dropout_39[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 32, 32, 86)   344         concatenate_36[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 32, 32, 86)   0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 32, 32, 14)   10836       activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_40 (Dropout)            (None, 32, 32, 14)   0           conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_37 (Concatenate)    (None, 32, 32, 100)  0           concatenate_36[0][0]             \n",
            "                                                                 dropout_40[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 32, 32, 100)  400         concatenate_37[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 32, 32, 100)  0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 32, 32, 14)   12600       activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_41 (Dropout)            (None, 32, 32, 14)   0           conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_38 (Concatenate)    (None, 32, 32, 114)  0           concatenate_37[0][0]             \n",
            "                                                                 dropout_41[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 32, 32, 114)  456         concatenate_38[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 32, 32, 114)  0           batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 32, 32, 14)   14364       activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_42 (Dropout)            (None, 32, 32, 14)   0           conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_39 (Concatenate)    (None, 32, 32, 128)  0           concatenate_38[0][0]             \n",
            "                                                                 dropout_42[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 32, 32, 128)  512         concatenate_39[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 32, 32, 128)  0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 32, 32, 14)   1792        activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_43 (Dropout)            (None, 32, 32, 14)   0           conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_4 (AveragePoo (None, 16, 16, 14)   0           dropout_43[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 16, 16, 14)   56          average_pooling2d_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 16, 16, 14)   0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 16, 16, 14)   1764        activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_44 (Dropout)            (None, 16, 16, 14)   0           conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_40 (Concatenate)    (None, 16, 16, 28)   0           average_pooling2d_4[0][0]        \n",
            "                                                                 dropout_44[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 16, 16, 28)   112         concatenate_40[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 16, 16, 28)   0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 16, 16, 14)   3528        activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_45 (Dropout)            (None, 16, 16, 14)   0           conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_41 (Concatenate)    (None, 16, 16, 42)   0           concatenate_40[0][0]             \n",
            "                                                                 dropout_45[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 16, 16, 42)   168         concatenate_41[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 16, 16, 42)   0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 16, 16, 14)   5292        activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_46 (Dropout)            (None, 16, 16, 14)   0           conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_42 (Concatenate)    (None, 16, 16, 56)   0           concatenate_41[0][0]             \n",
            "                                                                 dropout_46[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 16, 16, 56)   224         concatenate_42[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 16, 16, 56)   0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 16, 16, 14)   7056        activation_48[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_47 (Dropout)            (None, 16, 16, 14)   0           conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_43 (Concatenate)    (None, 16, 16, 70)   0           concatenate_42[0][0]             \n",
            "                                                                 dropout_47[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 16, 16, 70)   280         concatenate_43[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 16, 16, 70)   0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 16, 16, 14)   8820        activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_48 (Dropout)            (None, 16, 16, 14)   0           conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_44 (Concatenate)    (None, 16, 16, 84)   0           concatenate_43[0][0]             \n",
            "                                                                 dropout_48[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 16, 16, 84)   336         concatenate_44[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 16, 16, 84)   0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 16, 16, 14)   10584       activation_50[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_49 (Dropout)            (None, 16, 16, 14)   0           conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_45 (Concatenate)    (None, 16, 16, 98)   0           concatenate_44[0][0]             \n",
            "                                                                 dropout_49[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 16, 16, 98)   392         concatenate_45[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 16, 16, 98)   0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 16, 16, 14)   12348       activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_50 (Dropout)            (None, 16, 16, 14)   0           conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_46 (Concatenate)    (None, 16, 16, 112)  0           concatenate_45[0][0]             \n",
            "                                                                 dropout_50[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, 16, 16, 112)  448         concatenate_46[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 16, 16, 112)  0           batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 16, 16, 14)   14112       activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_51 (Dropout)            (None, 16, 16, 14)   0           conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_47 (Concatenate)    (None, 16, 16, 126)  0           concatenate_46[0][0]             \n",
            "                                                                 dropout_51[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_53 (BatchNo (None, 16, 16, 126)  504         concatenate_47[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, 16, 16, 126)  0           batch_normalization_53[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 16, 16, 14)   1764        activation_53[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_52 (Dropout)            (None, 16, 16, 14)   0           conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_5 (AveragePoo (None, 8, 8, 14)     0           dropout_52[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_54 (BatchNo (None, 8, 8, 14)     56          average_pooling2d_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_54 (Activation)      (None, 8, 8, 14)     0           batch_normalization_54[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 8, 8, 14)     1764        activation_54[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_53 (Dropout)            (None, 8, 8, 14)     0           conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_48 (Concatenate)    (None, 8, 8, 28)     0           average_pooling2d_5[0][0]        \n",
            "                                                                 dropout_53[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_55 (BatchNo (None, 8, 8, 28)     112         concatenate_48[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_55 (Activation)      (None, 8, 8, 28)     0           batch_normalization_55[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 8, 8, 14)     3528        activation_55[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_54 (Dropout)            (None, 8, 8, 14)     0           conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_49 (Concatenate)    (None, 8, 8, 42)     0           concatenate_48[0][0]             \n",
            "                                                                 dropout_54[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_56 (BatchNo (None, 8, 8, 42)     168         concatenate_49[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_56 (Activation)      (None, 8, 8, 42)     0           batch_normalization_56[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_57 (Conv2D)              (None, 8, 8, 14)     5292        activation_56[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_55 (Dropout)            (None, 8, 8, 14)     0           conv2d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_50 (Concatenate)    (None, 8, 8, 56)     0           concatenate_49[0][0]             \n",
            "                                                                 dropout_55[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_57 (BatchNo (None, 8, 8, 56)     224         concatenate_50[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_57 (Activation)      (None, 8, 8, 56)     0           batch_normalization_57[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_58 (Conv2D)              (None, 8, 8, 14)     7056        activation_57[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_56 (Dropout)            (None, 8, 8, 14)     0           conv2d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_51 (Concatenate)    (None, 8, 8, 70)     0           concatenate_50[0][0]             \n",
            "                                                                 dropout_56[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_58 (BatchNo (None, 8, 8, 70)     280         concatenate_51[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_58 (Activation)      (None, 8, 8, 70)     0           batch_normalization_58[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_59 (Conv2D)              (None, 8, 8, 14)     8820        activation_58[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_57 (Dropout)            (None, 8, 8, 14)     0           conv2d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_52 (Concatenate)    (None, 8, 8, 84)     0           concatenate_51[0][0]             \n",
            "                                                                 dropout_57[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_59 (BatchNo (None, 8, 8, 84)     336         concatenate_52[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_59 (Activation)      (None, 8, 8, 84)     0           batch_normalization_59[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_60 (Conv2D)              (None, 8, 8, 14)     10584       activation_59[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_58 (Dropout)            (None, 8, 8, 14)     0           conv2d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_53 (Concatenate)    (None, 8, 8, 98)     0           concatenate_52[0][0]             \n",
            "                                                                 dropout_58[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_60 (BatchNo (None, 8, 8, 98)     392         concatenate_53[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_60 (Activation)      (None, 8, 8, 98)     0           batch_normalization_60[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_61 (Conv2D)              (None, 8, 8, 14)     12348       activation_60[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_59 (Dropout)            (None, 8, 8, 14)     0           conv2d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_54 (Concatenate)    (None, 8, 8, 112)    0           concatenate_53[0][0]             \n",
            "                                                                 dropout_59[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_61 (BatchNo (None, 8, 8, 112)    448         concatenate_54[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_61 (Activation)      (None, 8, 8, 112)    0           batch_normalization_61[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_62 (Conv2D)              (None, 8, 8, 14)     14112       activation_61[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_60 (Dropout)            (None, 8, 8, 14)     0           conv2d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_55 (Concatenate)    (None, 8, 8, 126)    0           concatenate_54[0][0]             \n",
            "                                                                 dropout_60[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_62 (BatchNo (None, 8, 8, 126)    504         concatenate_55[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_62 (Activation)      (None, 8, 8, 126)    0           batch_normalization_62[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_63 (Conv2D)              (None, 8, 8, 14)     1764        activation_62[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_61 (Dropout)            (None, 8, 8, 14)     0           conv2d_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_6 (AveragePoo (None, 4, 4, 14)     0           dropout_61[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_63 (BatchNo (None, 4, 4, 14)     56          average_pooling2d_6[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_63 (Activation)      (None, 4, 4, 14)     0           batch_normalization_63[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_64 (Conv2D)              (None, 4, 4, 14)     1764        activation_63[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_62 (Dropout)            (None, 4, 4, 14)     0           conv2d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_56 (Concatenate)    (None, 4, 4, 28)     0           average_pooling2d_6[0][0]        \n",
            "                                                                 dropout_62[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_64 (BatchNo (None, 4, 4, 28)     112         concatenate_56[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_64 (Activation)      (None, 4, 4, 28)     0           batch_normalization_64[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_65 (Conv2D)              (None, 4, 4, 14)     3528        activation_64[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_63 (Dropout)            (None, 4, 4, 14)     0           conv2d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_57 (Concatenate)    (None, 4, 4, 42)     0           concatenate_56[0][0]             \n",
            "                                                                 dropout_63[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_65 (BatchNo (None, 4, 4, 42)     168         concatenate_57[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_65 (Activation)      (None, 4, 4, 42)     0           batch_normalization_65[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_66 (Conv2D)              (None, 4, 4, 14)     5292        activation_65[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_64 (Dropout)            (None, 4, 4, 14)     0           conv2d_66[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_58 (Concatenate)    (None, 4, 4, 56)     0           concatenate_57[0][0]             \n",
            "                                                                 dropout_64[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_66 (BatchNo (None, 4, 4, 56)     224         concatenate_58[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_66 (Activation)      (None, 4, 4, 56)     0           batch_normalization_66[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_67 (Conv2D)              (None, 4, 4, 14)     7056        activation_66[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_65 (Dropout)            (None, 4, 4, 14)     0           conv2d_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_59 (Concatenate)    (None, 4, 4, 70)     0           concatenate_58[0][0]             \n",
            "                                                                 dropout_65[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_67 (BatchNo (None, 4, 4, 70)     280         concatenate_59[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_67 (Activation)      (None, 4, 4, 70)     0           batch_normalization_67[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_68 (Conv2D)              (None, 4, 4, 14)     8820        activation_67[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_66 (Dropout)            (None, 4, 4, 14)     0           conv2d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_60 (Concatenate)    (None, 4, 4, 84)     0           concatenate_59[0][0]             \n",
            "                                                                 dropout_66[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_68 (BatchNo (None, 4, 4, 84)     336         concatenate_60[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_68 (Activation)      (None, 4, 4, 84)     0           batch_normalization_68[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_69 (Conv2D)              (None, 4, 4, 14)     10584       activation_68[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_67 (Dropout)            (None, 4, 4, 14)     0           conv2d_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_61 (Concatenate)    (None, 4, 4, 98)     0           concatenate_60[0][0]             \n",
            "                                                                 dropout_67[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_69 (BatchNo (None, 4, 4, 98)     392         concatenate_61[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_69 (Activation)      (None, 4, 4, 98)     0           batch_normalization_69[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_70 (Conv2D)              (None, 4, 4, 14)     12348       activation_69[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_68 (Dropout)            (None, 4, 4, 14)     0           conv2d_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_62 (Concatenate)    (None, 4, 4, 112)    0           concatenate_61[0][0]             \n",
            "                                                                 dropout_68[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_70 (BatchNo (None, 4, 4, 112)    448         concatenate_62[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_70 (Activation)      (None, 4, 4, 112)    0           batch_normalization_70[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_71 (Conv2D)              (None, 4, 4, 14)     14112       activation_70[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_69 (Dropout)            (None, 4, 4, 14)     0           conv2d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_63 (Concatenate)    (None, 4, 4, 126)    0           concatenate_62[0][0]             \n",
            "                                                                 dropout_69[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_71 (BatchNo (None, 4, 4, 126)    504         concatenate_63[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_71 (Activation)      (None, 4, 4, 126)    0           batch_normalization_71[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_7 (AveragePoo (None, 2, 2, 126)    0           activation_71[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 504)          0           average_pooling2d_7[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10)           5050        flatten_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 276,986\n",
            "Trainable params: 271,910\n",
            "Non-trainable params: 5,076\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LOFMqeu_7aA",
        "colab_type": "text"
      },
      "source": [
        "### Callback"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4XOsW3ahSkL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# determine Loss function and Optimizer\n",
        "LEARNING_RATE=0.4\n",
        "WEIGHT_DECAY=5e-4\n",
        "lr_schedule = lambda t: np.interp([t+1], [0, (epochs+1)//5, int(0.8*epochs), epochs], [0, LEARNING_RATE, 0.1*LEARNING_RATE, 0.005])[0]\n",
        "\n",
        "model.compile(optimizer=SGD(momentum=0.9, decay=WEIGHT_DECAY, nesterov=True), \n",
        "              loss='categorical_crossentropy', \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "!mkdir \"Assignment16/\"\n",
        "filepath=\"Assignment16/maxAccuracy.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [LearningRateScheduler(lr_schedule),checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbf09k-fC3Qt",
        "colab_type": "text"
      },
      "source": [
        "### Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzrHE-02OEgZ",
        "colab_type": "code",
        "outputId": "1cfe1050-9bbc-453f-8760-0ca5ed98a7fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# STEPS_PER_EPOCH = len(x_train) / batch_size\n",
        "start = time.time()\n",
        "model_info = model.fit(x_trainTF, y_trainTF,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_testTF, y_testTF),\n",
        "                    validation_steps = VALIDATION_STEPS, callbacks=callbacks_list)\n",
        "\n",
        "end = time.time()\n",
        "print (\"Model took %0.2f seconds to train\"%(end - start))\n",
        "# plot model history\n",
        "plot_model_history(model_info)\n",
        "# compute test accuracy\n",
        "print (\"Accuracy on test data is: %0.2f\"%accuracy(x_test, y_test, model1))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "390/390 [============================>.] - ETA: 0s - loss: 1.4700 - acc: 0.4630\n",
            "Epoch 00001: val_acc improved from -inf to 0.49684, saving model to Assignment16/maxAccuracy.hdf5\n",
            "391/390 [==============================] - 119s 305ms/step - loss: 1.4689 - acc: 0.4633 - val_loss: 1.4522 - val_acc: 0.4968\n",
            "Epoch 2/50\n",
            "390/390 [============================>.] - ETA: 0s - loss: 1.0867 - acc: 0.6093\n",
            "Epoch 00002: val_acc improved from 0.49684 to 0.60047, saving model to Assignment16/maxAccuracy.hdf5\n",
            "391/390 [==============================] - 99s 253ms/step - loss: 1.0865 - acc: 0.6095 - val_loss: 1.1837 - val_acc: 0.6005\n",
            "Epoch 3/50\n",
            "390/390 [============================>.] - ETA: 0s - loss: 0.8868 - acc: 0.6833\n",
            "Epoch 00003: val_acc improved from 0.60047 to 0.62619, saving model to Assignment16/maxAccuracy.hdf5\n",
            "391/390 [==============================] - 99s 254ms/step - loss: 0.8864 - acc: 0.6834 - val_loss: 1.1489 - val_acc: 0.6262\n",
            "Epoch 4/50\n",
            "390/390 [============================>.] - ETA: 0s - loss: 0.7691 - acc: 0.7289\n",
            "Epoch 00004: val_acc improved from 0.62619 to 0.64300, saving model to Assignment16/maxAccuracy.hdf5\n",
            "391/390 [==============================] - 99s 254ms/step - loss: 0.7685 - acc: 0.7291 - val_loss: 1.1456 - val_acc: 0.6430\n",
            "Epoch 5/50\n",
            "390/390 [============================>.] - ETA: 0s - loss: 0.6822 - acc: 0.7608\n",
            "Epoch 00005: val_acc did not improve from 0.64300\n",
            "391/390 [==============================] - 99s 253ms/step - loss: 0.6819 - acc: 0.7609 - val_loss: 1.4569 - val_acc: 0.6052\n",
            "Epoch 6/50\n",
            "390/390 [============================>.] - ETA: 0s - loss: 0.6271 - acc: 0.7795\n",
            "Epoch 00006: val_acc improved from 0.64300 to 0.73477, saving model to Assignment16/maxAccuracy.hdf5\n",
            "391/390 [==============================] - 99s 254ms/step - loss: 0.6270 - acc: 0.7795 - val_loss: 0.7672 - val_acc: 0.7348\n",
            "Epoch 7/50\n",
            "390/390 [============================>.] - ETA: 0s - loss: 0.5713 - acc: 0.7987\n",
            "Epoch 00007: val_acc did not improve from 0.73477\n",
            "391/390 [==============================] - 99s 253ms/step - loss: 0.5710 - acc: 0.7988 - val_loss: 1.1611 - val_acc: 0.6557\n",
            "Epoch 8/50\n",
            "390/390 [============================>.] - ETA: 0s - loss: 0.5388 - acc: 0.8106\n",
            "Epoch 00008: val_acc improved from 0.73477 to 0.76978, saving model to Assignment16/maxAccuracy.hdf5\n",
            "391/390 [==============================] - 99s 254ms/step - loss: 0.5386 - acc: 0.8107 - val_loss: 0.7112 - val_acc: 0.7698\n",
            "Epoch 9/50\n",
            "390/390 [============================>.] - ETA: 0s - loss: 0.5026 - acc: 0.8250\n",
            "Epoch 00009: val_acc did not improve from 0.76978\n",
            "391/390 [==============================] - 99s 253ms/step - loss: 0.5025 - acc: 0.8250 - val_loss: 0.7048 - val_acc: 0.7595\n",
            "Epoch 10/50\n",
            "390/390 [============================>.] - ETA: 0s - loss: 0.4737 - acc: 0.8342\n",
            "Epoch 00010: val_acc improved from 0.76978 to 0.80251, saving model to Assignment16/maxAccuracy.hdf5\n",
            "391/390 [==============================] - 99s 254ms/step - loss: 0.4735 - acc: 0.8344 - val_loss: 0.5930 - val_acc: 0.8025\n",
            "Epoch 11/50\n",
            "390/390 [============================>.] - ETA: 0s - loss: 0.4355 - acc: 0.8474\n",
            "Epoch 00011: val_acc improved from 0.80251 to 0.82081, saving model to Assignment16/maxAccuracy.hdf5\n",
            "391/390 [==============================] - 99s 254ms/step - loss: 0.4352 - acc: 0.8474 - val_loss: 0.5726 - val_acc: 0.8208\n",
            "Epoch 12/50\n",
            "390/390 [============================>.] - ETA: 0s - loss: 0.4012 - acc: 0.8589\n",
            "Epoch 00012: val_acc did not improve from 0.82081\n",
            "391/390 [==============================] - 99s 253ms/step - loss: 0.4011 - acc: 0.8589 - val_loss: 0.7465 - val_acc: 0.7673\n",
            "Epoch 13/50\n",
            "390/390 [============================>.] - ETA: 0s - loss: 0.3718 - acc: 0.8687\n",
            "Epoch 00013: val_acc did not improve from 0.82081\n",
            "391/390 [==============================] - 99s 253ms/step - loss: 0.3716 - acc: 0.8688 - val_loss: 0.7677 - val_acc: 0.7672\n",
            "Epoch 14/50\n",
            "390/390 [============================>.] - ETA: 0s - loss: 0.3442 - acc: 0.8783\n",
            "Epoch 00014: val_acc did not improve from 0.82081\n",
            "391/390 [==============================] - 99s 253ms/step - loss: 0.3440 - acc: 0.8783 - val_loss: 0.5876 - val_acc: 0.8159\n",
            "Epoch 15/50\n",
            "390/390 [============================>.] - ETA: 0s - loss: 0.3220 - acc: 0.8864\n",
            "Epoch 00015: val_acc did not improve from 0.82081\n",
            "391/390 [==============================] - 99s 252ms/step - loss: 0.3221 - acc: 0.8864 - val_loss: 0.5915 - val_acc: 0.8199\n",
            "Epoch 16/50\n",
            "390/390 [============================>.] - ETA: 0s - loss: 0.3025 - acc: 0.8935\n",
            "Epoch 00016: val_acc improved from 0.82081 to 0.83564, saving model to Assignment16/maxAccuracy.hdf5\n",
            "391/390 [==============================] - 99s 253ms/step - loss: 0.3022 - acc: 0.8936 - val_loss: 0.5403 - val_acc: 0.8356\n",
            "Epoch 17/50\n",
            "390/390 [============================>.] - ETA: 0s - loss: 0.2817 - acc: 0.9002\n",
            "Epoch 00017: val_acc did not improve from 0.83564\n",
            "391/390 [==============================] - 98s 252ms/step - loss: 0.2817 - acc: 0.9002 - val_loss: 0.5516 - val_acc: 0.8347\n",
            "Epoch 18/50\n",
            "390/390 [============================>.] - ETA: 0s - loss: 0.2629 - acc: 0.9063\n",
            "Epoch 00018: val_acc did not improve from 0.83564\n",
            "391/390 [==============================] - 99s 253ms/step - loss: 0.2630 - acc: 0.9063 - val_loss: 0.6312 - val_acc: 0.8231\n",
            "Epoch 19/50\n",
            "390/390 [============================>.] - ETA: 0s - loss: 0.2502 - acc: 0.9120\n",
            "Epoch 00019: val_acc did not improve from 0.83564\n",
            "391/390 [==============================] - 99s 254ms/step - loss: 0.2501 - acc: 0.9121 - val_loss: 0.5858 - val_acc: 0.8224\n",
            "Epoch 20/50\n",
            "390/390 [============================>.] - ETA: 0s - loss: 0.2392 - acc: 0.9148\n",
            "Epoch 00020: val_acc did not improve from 0.83564\n",
            "391/390 [==============================] - 99s 254ms/step - loss: 0.2391 - acc: 0.9148 - val_loss: 0.5648 - val_acc: 0.8334\n",
            "Epoch 21/50\n",
            "390/390 [============================>.] - ETA: 0s - loss: 0.2196 - acc: 0.9219\n",
            "Epoch 00021: val_acc did not improve from 0.83564\n",
            "391/390 [==============================] - 99s 254ms/step - loss: 0.2196 - acc: 0.9220 - val_loss: 0.7000 - val_acc: 0.8095\n",
            "Epoch 22/50\n",
            "390/390 [============================>.] - ETA: 0s - loss: 0.2109 - acc: 0.9255\n",
            "Epoch 00022: val_acc improved from 0.83564 to 0.84751, saving model to Assignment16/maxAccuracy.hdf5\n",
            "391/390 [==============================] - 100s 255ms/step - loss: 0.2107 - acc: 0.9255 - val_loss: 0.5367 - val_acc: 0.8475\n",
            "Epoch 23/50\n",
            "390/390 [============================>.] - ETA: 0s - loss: 0.1954 - acc: 0.9302\n",
            "Epoch 00023: val_acc improved from 0.84751 to 0.84869, saving model to Assignment16/maxAccuracy.hdf5\n",
            "391/390 [==============================] - 100s 255ms/step - loss: 0.1954 - acc: 0.9302 - val_loss: 0.5367 - val_acc: 0.8487\n",
            "Epoch 24/50\n",
            "390/390 [============================>.] - ETA: 0s - loss: 0.1847 - acc: 0.9344\n",
            "Epoch 00024: val_acc did not improve from 0.84869\n",
            "391/390 [==============================] - 99s 254ms/step - loss: 0.1846 - acc: 0.9345 - val_loss: 0.5489 - val_acc: 0.8458\n",
            "Epoch 25/50\n",
            "390/390 [============================>.] - ETA: 0s - loss: 0.1755 - acc: 0.9381\n",
            "Epoch 00025: val_acc did not improve from 0.84869\n",
            "391/390 [==============================] - 99s 254ms/step - loss: 0.1754 - acc: 0.9382 - val_loss: 0.6607 - val_acc: 0.8278\n",
            "Epoch 26/50\n",
            "390/390 [============================>.] - ETA: 0s - loss: 0.1690 - acc: 0.9401\n",
            "Epoch 00026: val_acc did not improve from 0.84869\n",
            "391/390 [==============================] - 99s 254ms/step - loss: 0.1691 - acc: 0.9401 - val_loss: 0.5791 - val_acc: 0.8469\n",
            "Epoch 27/50\n",
            "390/390 [============================>.] - ETA: 0s - loss: 0.1583 - acc: 0.9422\n",
            "Epoch 00027: val_acc improved from 0.84869 to 0.85216, saving model to Assignment16/maxAccuracy.hdf5\n",
            "391/390 [==============================] - 100s 255ms/step - loss: 0.1581 - acc: 0.9423 - val_loss: 0.5510 - val_acc: 0.8522\n",
            "Epoch 28/50\n",
            "390/390 [============================>.] - ETA: 0s - loss: 0.1470 - acc: 0.9479\n",
            "Epoch 00028: val_acc improved from 0.85216 to 0.85908, saving model to Assignment16/maxAccuracy.hdf5\n",
            "391/390 [==============================] - 100s 255ms/step - loss: 0.1471 - acc: 0.9479 - val_loss: 0.5526 - val_acc: 0.8591\n",
            "Epoch 29/50\n",
            "390/390 [============================>.] - ETA: 0s - loss: 0.1417 - acc: 0.9489\n",
            "Epoch 00029: val_acc improved from 0.85908 to 0.86303, saving model to Assignment16/maxAccuracy.hdf5\n",
            "391/390 [==============================] - 100s 255ms/step - loss: 0.1415 - acc: 0.9490 - val_loss: 0.5269 - val_acc: 0.8630\n",
            "Epoch 30/50\n",
            "390/390 [============================>.] - ETA: 0s - loss: 0.1344 - acc: 0.9526\n",
            "Epoch 00030: val_acc did not improve from 0.86303\n",
            "391/390 [==============================] - 99s 254ms/step - loss: 0.1343 - acc: 0.9526 - val_loss: 0.7112 - val_acc: 0.8380\n",
            "Epoch 31/50\n",
            "390/390 [============================>.] - ETA: 0s - loss: 0.1294 - acc: 0.9536\n",
            "Epoch 00031: val_acc did not improve from 0.86303\n",
            "391/390 [==============================] - 99s 254ms/step - loss: 0.1293 - acc: 0.9536 - val_loss: 0.5606 - val_acc: 0.8617\n",
            "Epoch 32/50\n",
            "390/390 [============================>.] - ETA: 0s - loss: 0.1233 - acc: 0.9562\n",
            "Epoch 00032: val_acc did not improve from 0.86303\n",
            "391/390 [==============================] - 99s 254ms/step - loss: 0.1232 - acc: 0.9562 - val_loss: 0.5426 - val_acc: 0.8606\n",
            "Epoch 33/50\n",
            "390/390 [============================>.] - ETA: 0s - loss: 0.1182 - acc: 0.9579\n",
            "Epoch 00033: val_acc did not improve from 0.86303\n",
            "391/390 [==============================] - 99s 254ms/step - loss: 0.1181 - acc: 0.9580 - val_loss: 0.5791 - val_acc: 0.8574\n",
            "Epoch 34/50\n",
            "390/390 [============================>.] - ETA: 0s - loss: 0.1134 - acc: 0.9596\n",
            "Epoch 00034: val_acc did not improve from 0.86303\n",
            "391/390 [==============================] - 99s 254ms/step - loss: 0.1135 - acc: 0.9596 - val_loss: 0.5798 - val_acc: 0.8590\n",
            "Epoch 35/50\n",
            "390/390 [============================>.] - ETA: 0s - loss: 0.1082 - acc: 0.9612\n",
            "Epoch 00035: val_acc improved from 0.86303 to 0.86412, saving model to Assignment16/maxAccuracy.hdf5\n",
            "391/390 [==============================] - 100s 255ms/step - loss: 0.1081 - acc: 0.9613 - val_loss: 0.5662 - val_acc: 0.8641\n",
            "Epoch 36/50\n",
            "390/390 [============================>.] - ETA: 0s - loss: 0.1044 - acc: 0.9625\n",
            "Epoch 00036: val_acc improved from 0.86412 to 0.86768, saving model to Assignment16/maxAccuracy.hdf5\n",
            "391/390 [==============================] - 100s 255ms/step - loss: 0.1045 - acc: 0.9624 - val_loss: 0.5528 - val_acc: 0.8677\n",
            "Epoch 37/50\n",
            "390/390 [============================>.] - ETA: 0s - loss: 0.1001 - acc: 0.9648\n",
            "Epoch 00037: val_acc did not improve from 0.86768\n",
            "391/390 [==============================] - 99s 254ms/step - loss: 0.1001 - acc: 0.9648 - val_loss: 0.5634 - val_acc: 0.8649\n",
            "Epoch 38/50\n",
            "390/390 [============================>.] - ETA: 0s - loss: 0.0937 - acc: 0.9664\n",
            "Epoch 00038: val_acc did not improve from 0.86768\n",
            "391/390 [==============================] - 99s 254ms/step - loss: 0.0937 - acc: 0.9664 - val_loss: 0.5363 - val_acc: 0.8677\n",
            "Epoch 39/50\n",
            "390/390 [============================>.] - ETA: 0s - loss: 0.0931 - acc: 0.9670\n",
            "Epoch 00039: val_acc improved from 0.86768 to 0.86917, saving model to Assignment16/maxAccuracy.hdf5\n",
            "391/390 [==============================] - 100s 255ms/step - loss: 0.0931 - acc: 0.9669 - val_loss: 0.5746 - val_acc: 0.8692\n",
            "Epoch 40/50\n",
            "390/390 [============================>.] - ETA: 0s - loss: 0.0898 - acc: 0.9677\n",
            "Epoch 00040: val_acc did not improve from 0.86917\n",
            "391/390 [==============================] - 99s 254ms/step - loss: 0.0898 - acc: 0.9678 - val_loss: 0.5846 - val_acc: 0.8664\n",
            "Epoch 41/50\n",
            "390/390 [============================>.] - ETA: 0s - loss: 0.0897 - acc: 0.9680\n",
            "Epoch 00041: val_acc improved from 0.86917 to 0.87114, saving model to Assignment16/maxAccuracy.hdf5\n",
            "391/390 [==============================] - 99s 254ms/step - loss: 0.0898 - acc: 0.9680 - val_loss: 0.5655 - val_acc: 0.8711\n",
            "Epoch 42/50\n",
            "390/390 [============================>.] - ETA: 0s - loss: 0.0909 - acc: 0.9678\n",
            "Epoch 00042: val_acc did not improve from 0.87114\n",
            "391/390 [==============================] - 99s 253ms/step - loss: 0.0908 - acc: 0.9678 - val_loss: 0.5604 - val_acc: 0.8676\n",
            "Epoch 43/50\n",
            "390/390 [============================>.] - ETA: 0s - loss: 0.0864 - acc: 0.9699\n",
            "Epoch 00043: val_acc did not improve from 0.87114\n",
            "391/390 [==============================] - 99s 254ms/step - loss: 0.0864 - acc: 0.9699 - val_loss: 0.5700 - val_acc: 0.8684\n",
            "Epoch 44/50\n",
            "390/390 [============================>.] - ETA: 0s - loss: 0.0870 - acc: 0.9693\n",
            "Epoch 00044: val_acc did not improve from 0.87114\n",
            "391/390 [==============================] - 99s 254ms/step - loss: 0.0871 - acc: 0.9692 - val_loss: 0.5777 - val_acc: 0.8671\n",
            "Epoch 45/50\n",
            "390/390 [============================>.] - ETA: 0s - loss: 0.0848 - acc: 0.9700\n",
            "Epoch 00045: val_acc did not improve from 0.87114\n",
            "391/390 [==============================] - 99s 254ms/step - loss: 0.0848 - acc: 0.9700 - val_loss: 0.5777 - val_acc: 0.8667\n",
            "Epoch 46/50\n",
            "390/390 [============================>.] - ETA: 0s - loss: 0.0849 - acc: 0.9704\n",
            "Epoch 00046: val_acc improved from 0.87114 to 0.87253, saving model to Assignment16/maxAccuracy.hdf5\n",
            "391/390 [==============================] - 99s 254ms/step - loss: 0.0849 - acc: 0.9703 - val_loss: 0.5683 - val_acc: 0.8725\n",
            "Epoch 47/50\n",
            "390/390 [============================>.] - ETA: 0s - loss: 0.0862 - acc: 0.9696\n",
            "Epoch 00047: val_acc did not improve from 0.87253\n",
            "391/390 [==============================] - 99s 253ms/step - loss: 0.0861 - acc: 0.9696 - val_loss: 0.5820 - val_acc: 0.8661\n",
            "Epoch 48/50\n",
            "390/390 [============================>.] - ETA: 0s - loss: 0.0833 - acc: 0.9709\n",
            "Epoch 00048: val_acc did not improve from 0.87253\n",
            "391/390 [==============================] - 99s 254ms/step - loss: 0.0834 - acc: 0.9709 - val_loss: 0.5758 - val_acc: 0.8685\n",
            "Epoch 49/50\n",
            "390/390 [============================>.] - ETA: 0s - loss: 0.0804 - acc: 0.9724\n",
            "Epoch 00049: val_acc did not improve from 0.87253\n",
            "391/390 [==============================] - 99s 254ms/step - loss: 0.0804 - acc: 0.9723 - val_loss: 0.5639 - val_acc: 0.8714\n",
            "Epoch 50/50\n",
            "390/390 [============================>.] - ETA: 0s - loss: 0.0834 - acc: 0.9706\n",
            "Epoch 00050: val_acc did not improve from 0.87253\n",
            "391/390 [==============================] - 99s 253ms/step - loss: 0.0833 - acc: 0.9706 - val_loss: 0.5783 - val_acc: 0.8680\n",
            "Model took 4990.40 seconds to train\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-7b8098f4c915>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Model took %0.2f seconds to train\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# plot model history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mplot_model_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;31m# compute test accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy on test data is: %0.2f\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-e05d9b01600e>\u001b[0m in \u001b[0;36mplot_model_history\u001b[0;34m(model_history)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_model_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;31m# summarize history for accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0maxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0maxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJRFQ15V9emo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "30a34f9d-6707-4903-a96e-6a5ffc653475"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plot_model_history(model_info)\n",
        "print (\"Accuracy on test data is: %0.2f\"%accuracy(x_test, y_test, model))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3sAAAFNCAYAAAC5cXZ6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBoEHDml11akAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1080x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy on test data is: 10.40\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHBzYFg2RmqV",
        "colab_type": "text"
      },
      "source": [
        "### Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcWydmIVhZGr",
        "colab_type": "code",
        "outputId": "bbded798-3591-46f6-faea-d208011f2394",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# Test the model\n",
        "model.load_weights('./Assignment16/maxAccuracy.hdf5')\n",
        "score = model.evaluate(x_testTF, y_testTF, verbose=1, steps = VALIDATION_STEPS)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "79/78 [==============================] - 5s 60ms/step - loss: 0.5706 - acc: 0.8702\n",
            "Test loss: 0.5769504623413086\n",
            "Test accuracy: 0.87015426\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}